# =========================================================
# FILE: cicd_pipeline.yml
# GitHub Actions Workflow for Databricks CI/CD Automation
# =========================================================

name: Databricks CI/CD Pipeline

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  build-and-deploy:
    name: Build & Deploy (${{ matrix.env }})
    runs-on: ubuntu-latest

    strategy:
      matrix:
        env: [dev, qa, prod]

    environment: ${{ matrix.env }}

    steps:
      # ---------- CI STEPS ----------
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.9
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: Cache Python dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install databricks-cli

      - name: Run static analysis
        run: pylint notebooks/*.py

      - name: Run unit tests
        run: pytest tests/

      - name: Validate Databricks Asset Bundle
        run: databricks bundle validate

      # ---------- CD STEPS ----------
      - name: Azure Login (OIDC)
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          client-secret: ${{ secrets.AZURE_CLIENT_SECRET }}

      - name: Deploy Databricks Asset Bundle
        id: deploy
        run: databricks bundle deploy --target ${{ matrix.env }}

      - name: Run smoke tests
        run: python scripts/smoke_tests.py

      - name: Optimize Delta tables post-deployment
        run: databricks sql "OPTIMIZE retail_sales_table"

      # ---------- ROLLBACK ----------
      - name: Rollback on failure
        if: failure()
        run: databricks bundle deploy --rollback --target ${{ matrix.env }}

      # ---------- SLACK NOTIFICATION ----------
      - name: Send Slack notification
        uses: slackapi/slack-github-action@v1.25.0
        with:
          payload: |
            {
              "text": "Databricks deployment in ${{ matrix.env }} completed with status ${{ job.status }}."
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

      # ---------- STRUCTURED JSON LOGGING ----------
      - name: Structured JSON log - Deploy
        run: |
          python scripts/log_step.py \
            --step_name="Deploy Databricks Asset Bundle" \
            --environment="${{ matrix.env }}" \
            --result="${{ steps.deploy.outcome }}" \
            --timestamp="$(date --utc +%FT%TZ)"

      # ---------- PROD APPROVAL GATE ----------
      - name: Approval for prod
        if: matrix.env == 'prod'
        uses: trstringer/manual-approval@v1
        with:
          secret: ${{ secrets.GITHUB_TOKEN }}
          approvers: user1,user2 # <-- Update with actual approvers

# ====================
# POST-DEPLOYMENT TRIGGER EXAMPLES
# ====================

# --- OPTION 1: SAME REPO WORKFLOW_RUN TRIGGER ---
# In a separate workflow:
#
# on:
#   workflow_run:
#     workflows: ["Databricks CI/CD Pipeline"]
#     types:
#       - completed
#
# jobs:
#   on-success:
#     if: ${{ github.event.workflow_run.conclusion == 'success' }}
#     runs-on: ubuntu-latest
#     steps:
#       - name: Do post-deployment stuff
#         run: echo "Post-deployment triggered!"

# --- OPTION 2: CROSS-REPO TRIGGER ---
# Add as the last step in this workflow:
#
# - name: Trigger post-deploy in target repo
#   uses: peter-evans/repository-dispatch@v3
#   with:
#     token: ${{ secrets.GITHUB_TOKEN }}
#     repository: your-org/target-repo
#     event-type: post-deploy-trigger
#     client-payload: '{"environment":"${{ matrix.env }}"}'

# =========================================================
# FILE: databricks.yml
# Databricks Asset Bundle Configuration
# =========================================================

bundle:
  name: retail_analytics_bundle
  # tags:
  #   project: retail-analytics
  # description: >
  #   Bundle for retail analytics workflows and assets.

targets:
  dev: &dev_target
    workspace:
      host: https://<dev-workspace-url>
    default: true
    variables:
      env_name: dev
      data_path: /mnt/dev/data

  qa: &qa_target
    workspace:
      host: https://<qa-workspace-url>
    variables:
      env_name: qa
      data_path: /mnt/qa/data

  prod: &prod_target
    workspace:
      host: https://<prod-workspace-url>
    variables:
      env_name: prod
      data_path: /mnt/prod/data

resources:
  jobs:
    retail_processing_job:
      name: Retail Processing Job
      # Main ETL job for processing retail data
      tasks:
        - task_key: retail_processing_task
          notebook_task:
            notebook_path: notebooks/retail_processing.py
          cluster:
            spark_version: 13.3.x-scala2.12
            node_type_id: Standard_DS3_v2
            num_workers: 1

  sql_objects:
    delta_tables:
      - name: retail_sales_table
        full_name: ${var.data_path}/retail_sales
        comment: Retail sales data table
        permissions:
          - principal: analyst@example.com
            action: SELECT

variables:
  # Global variables (override in targets if needed)
  # example_global_var: some_value

secrets:
  # Define secret scopes if required for notebooks/jobs
  # Example:
  # - scope: my-secret-scope
  #   key: DATABRICKS_TOKEN

# =======================
# NOTES:
# - Use YAML anchors (&dev_target, *dev_target) to reuse target properties if desired.
# - For more settings, see: https://docs.databricks.com/en/dev-tools/bundles/settings.html
# - Replace placeholder workspace URLs and emails as needed.
# - This file is version-controlled (commit to repo).
# =======================
