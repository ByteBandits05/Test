# =========================================================
# FILE: databricks.yml
# Databricks Asset Bundle Configuration
# =========================================================

bundle:
  name: retail_analytics_bundle
  # tags:
  #   project: retail-analytics
  # description: >
  #   Bundle for retail analytics workflows and assets.

targets:
  dev: &dev_target
    workspace:
      host: https://<dev-workspace-url>
    default: true
    variables:
      env_name: dev
      data_path: /mnt/dev/data

  qa: &qa_target
    workspace:
      host: https://<qa-workspace-url>
    variables:
      env_name: qa
      data_path: /mnt/qa/data

  prod: &prod_target
    workspace:
      host: https://<prod-workspace-url>
    variables:
      env_name: prod
      data_path: /mnt/prod/data

resources:
  jobs:
    retail_processing_job:
      name: Retail Processing Job
      # Main ETL job for processing retail data
      tasks:
        - task_key: retail_processing_task
          notebook_task:
            notebook_path: notebooks/retail_processing.py
          cluster:
            spark_version: 13.3.x-scala2.12
            node_type_id: Standard_DS3_v2
            num_workers: 1

  sql_objects:
    delta_tables:
      - name: retail_sales_table
        full_name: ${var.data_path}/retail_sales
        comment: Retail sales data table
        permissions:
          - principal: analyst@example.com
            action: SELECT

variables:
  # Global variables (override in targets if needed)
  # example_global_var: some_value

secrets:
  # Define secret scopes if required for notebooks/jobs
  # Example:
  # - scope: my-secret-scope
  #   key: DATABRICKS_TOKEN

# =======================
# NOTES:
# - Use YAML anchors (&dev_target, *dev_target) to reuse target properties if desired.
# - For more settings, see: https://docs.databricks.com/en/dev-tools/bundles/settings.html
# - Replace placeholder workspace URLs and emails as needed.
# - This file is version-controlled (commit to repo).
# =======================
